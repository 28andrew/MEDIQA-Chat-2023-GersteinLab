# CONFIT: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning

We propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called ConFiT. Based on our linguistically-informed typology of errors, we design different modular objectives that each target a specific type.

The code is based on huggaface's [transformers](https://github.com/huggingface/transformers). Thanks to them! 

## Test

```
from transformers import PegasusTokenizer, PegasusForConditionalGeneration
model = PegasusForConditionalGeneration.from_pretrained('FROM_OUR_PRETRAINED_MODELS')
device = "cuda:0"
model = model.to(device)
tokenizer = PegasusTokenizer.from_pretrained('google/pegasus-xsum')
ARTICLE_TO_SUMMARIZE = ('"#ENTER")
inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')
print (inputs['input_ids'].shape)
```


## Annotation

[Google Drive](https://drive.google.com/drive/folders/1-IkrMDvDoBaEq9Y_ghSyxaAQIQRNCUbu?usp=sharing) is the unblinded annotation files that I used to compute the distributions in Tables 3 and 4. 

And see analysis.ipynb: the script to calculate the score.

## Generate Summary

```
import torch
input_ids = torch.load('./input_ids.pt')
decoder_input_ids = torch.load('./decoder_input_ids.pt')
attention_mask = torch.load('./attention_mask.pt')
print (input_ids.shape)
print (decoder_input_ids.shape)
print (attention_mask.shape)
output = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
```

## Released Model Checkpoints
We have released the following checkpoints for our pre-trained models:

* **T5-CONFIT**: [Google Drive](https://drive.google.com/drive/folders/1zCHIGAHifJ1oH00yNLkUsfWE14aGUpe7?usp=sharing)
* **Pegasus-CONFIT**: [Google Drive](https://drive.google.com/drive/folders/1uikCqSNMEVK2vKOfShoC6FmU-fH5cGMh?usp=sharing)
* **BART-CONFIT**: [Google Drive](https://drive.google.com/drive/folders/1pbqfHxs3oa38lLJw7j1mWDtpaWYfL2UD?usp=sharing)

## Citation
```
@inproceedings{tang-etal-2022-confit,
    title = "{CONFIT}: Toward Faithful Dialogue Summarization with Linguistically-Informed Contrastive Fine-tuning",
    author = "Tang, Xiangru  and
      Nair, Arjun  and
      Wang, Borui  and
      Wang, Bingyao  and
      Desai, Jai  and
      Wade, Aaron  and
      Li, Haoran  and
      Celikyilmaz, Asli  and
      Mehdad, Yashar  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.415",
    pages = "5657--5668",
    abstract = "Factual inconsistencies in generated summaries severely limit the practical applications of abstractive dialogue summarization. Although significant progress has been achieved by using pre-trained neural language models, substantial amounts of hallucinated content are found during the human evaluation. In this work, we first devised a typology of factual errors to better understand the types of hallucinations generated by current models and conducted human evaluation on popular dialog summarization dataset. We further propose a training strategy that improves the factual consistency and overall quality of summaries via a novel contrastive fine-tuning, called CONFIT. To tackle top factual errors from our annotation, we introduce additional contrastive loss with carefully designed hard negative samples and self-supervised dialogue-specific loss to capture the key information between speakers. We show that our model significantly reduces all kinds of factual errors on both SAMSum dialogue summarization and AMI meeting summarization. On both datasets, we achieve significant improvements over state-of-the-art baselines using both automatic metrics, ROUGE and BARTScore, and human evaluation.",
}
```
